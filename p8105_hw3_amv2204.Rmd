---
title: "Homework 3"
author: "Ashwini Varghese"
date: "2022-10-17"
output: github_document
editor_options: 
  chunk_output_type: console
---

## Setup for coding

```{r setup, include = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_color_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1



## Problem 2

Let's load, tidy, and wrangling this dataset:

```{r accel_cleaning}
accel = read_csv("./accel_data.csv") %>% 
    janitor::clean_names() %>% 
    pivot_longer(
      activity_1:activity_1440,
      names_to = "minute",
      names_prefix = "activity_",
      values_to = "activity_count"
    ) %>% 
    mutate(day_type = if_else(day == "Saturday", "weekend",
                              if_else(day == "Sunday", "weekend", "weekday"))) %>%
    mutate(day = as.factor(day)) %>% 
    mutate(minute = as.numeric(minute)) %>% 
    mutate(day_type = as.factor(day_type)) %>% 
    mutate(day = fct_relevel(day, "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

accel
```

Description of dataset:

This dataset has 6 variable (`ncol(accel)`); `week`, `day`, and `day_id` were from the original file untouched besides cleaning up the names using `janitor::clean_names()` but the other 3 (`minute,` `activity_count`, and `day_type`) are new variables created using `pivot_longer` and `mutate`. This dataset has 50,400 observations, found using `nrow(accel)`.


Let's now make a new variable that is the total activity count for a given day and show it in a table:

```{r total_activity}
accel2 <- accel %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity) %>% 
  knitr::kable(digits = 1)

accel2
```

From this table, it is hard to determine if any trends are apparent over the days and by each week.

So let's then make a single panel that shows the 24-hour activity time courses for each day:

```{r accel_plot}
accel %>%
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_point(aes(alpha = .5)) 
```

From this graph, we can see that the activity counts throughout any given day are usually not higher than about 1,875. The exceptions are at about the 400th minute, 560th minute, 650th minute, 1000th minute and 1,250th minute. The days of the week that we see these higher activity counts are on Thursdays, Fridays, Sundays, and Saturdays, respectively. At the 1,250th minute, on various days throughout the week, there is a large activity count. 

## Problem 3

Let's load the data:

```{r load_nynoaa}
library(p8105.datasets)
data("ny_noaa")

ny_noaa
```

The dataset has 7 variables (`ncol(ny_noaa)`) and 2595176 observations (`nrow(ny_noaa)`). It has a combination of integer and character variables, with also a date variable. The variables that make up the dataset are an ID variable of the weather station, date of observation, precipitation (mm), snowfall (mm), snow depth (mm), and the maximum and minimum temperatures in Celsius. There is a large number of missing data because each weather station may collect only a subset of these variables, so the dataset has observations with missing data. 

Let's now do some data cleaning of this dataset:

```{r clean_nynoaa}
set.seed(100)
ny_noaa_samp = 
  ny_noaa %>% 
  sample_n(2000) %>% 
  janitor::clean_names() %>%
  separate(date, sep = "-", into = c("year", "month", "day")) %>%
  mutate_at(c(2:9), as.numeric) %>% 
  mutate(prcp = prcp/10) %>% 
  mutate(tmin = tmin/10, tmax = tmax/10) %>% 
  mutate(month = month.name[as.numeric(month)])
ny_noaa_samp
```

We cleaned up the data by cleaning the names, separating the variable for date of observation into the year, month, and day, converting all the variables except the ID into a numeric variable, and converting the `prcp`, `tmix`, and `tmax` variables from it's tenths value to it's whole value by dividing by 10.  

We will next find the most commonly observed value for `snowfall` using the `count` and `order` functions. 

```{r snowfall}

snow_df <- ny_noaa %>% 
  count(snow, name = "n_obs")
  
snow_df <- snow_df[order(-snow_df$n_obs),] %>% 
  knitr::kable(digits = 1)

snow_df #Should kable this??
```

We can see from the resulting table that the most observed value for `snowfall` is 0 mm. After that, we have 25 mm and the third highest value is 13 mm. These are the highest values maybe because the observations in this dataset were collected from weather stations in New York state and for a majority of the year, the weather is not optimal for snow.

Now let's make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r two_plot}
avg <- ny_noaa_samp %>% 
  filter(
    month %in% c("January", "July")
  ) %>% 
  drop_na(tmax) %>% 
  group_by(year, id, month) %>% 
  summarize(
    avg_tmax = mean(tmax, na.rm = TRUE)
  ) %>%
  ggplot(aes(x = year, y = avg_tmax, group = id)) +
  geom_line() +
  geom_point() +
  facet_grid(. ~ month) +
  theme(legend.position = "none")
  
```










Code with full dataset to add at end:

ny_noaa_clean <- ny_noaa %>% 
  janitor::clean_names() %>%
  separate(date, sep = "-", into = c("year", "month", "day")) 
  
