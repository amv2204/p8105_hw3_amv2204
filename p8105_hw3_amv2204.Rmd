---
title: "Homework 3"
author: "Ashwini Varghese"
date: "2022-10-15"
output: github_document
editor_options: 
  chunk_output_type: console
---

## Setup for coding

```{r setup, include = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_color_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1



## Problem 2

Let's load, tidy, and wrangling this dataset:

```{r clean}
accel = read_csv("./accel_data.csv") %>% 
    janitor::clean_names() %>% 
    pivot_longer(
      activity_1:activity_1440,
      names_to = "minute",
      names_prefix = "activity_",
      values_to = "activity_count"
    ) %>% 
    mutate(day_type = if_else(day == "Saturday", "weekend",
                              if_else(day == "Sunday", "weekend", "weekday"))) %>%
    mutate(day = as.factor(day)) %>% 
    mutate(minute = as.numeric(minute)) %>% 
    mutate(day_type = as.factor(day_type)) %>% 
    mutate(day = fct_relevel(day, "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

accel
```

Description of dataset:

This dataset has 6 variable (`ncol(accel)`); week, day, and day_id were from the original file untouched besides cleaning up the names using `janitor::clean_names()` but the other 3 (minute, activity_count, and day_type) are new variables created using `pivot_longer` and `mutate`. This dataset has 50,400 observations, found using `nrow(accel)`.


Let's now make a new variable that is the total activity count for a given day and show it in a table:

```{r total_activity}
accel2 <- accel %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity) %>% 
  knitr::kable(digits = 1)

accel2
```

From this table, it is hard to determine if any trends are apparent over the days and by each week.

So let's then make a single panel that shows the 24-hour activity time courses for each day:

```{r}
accel %>%
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_point(aes(alpha = .5)) 
```

From this graph, we can see that the activity counts throughout any given day are usually not higher than about 1,875. The exceptions are at about the 400th minute, 560th minute, 650th minute, 1000th minute and 1,250th minute. The days of the week that we see these higher activity counts are on Thursdays, Fridays, Sundays, and Saturdays, respectively. At the 1,250th minute, on various days throughout the week, there is a large activity count. 

## Problem 3

Let's load the data:

```{r}
library(p8105.datasets)
data("ny_noaa")

ny_noaa
```

The dataset has 7 variables (`ncol(ny_noaa)`) and 2595176 observations (`nrow(ny_noaa)`). It has a combination of integer and character variables, with also a date variable. The variables that make up the dataset are an ID variable of the weather station, date of observation, precipitation (mm), snowfall (mm), snow depth (mm), and the maximum and minimum temperatures in Celsius. There is a large number of missing data because each weather station may collect only a subset of these variables, so the dataset has observations with missing data. 

Let's now do some data cleaning of this dataset:

```{r}
ny_noaa_samp = 
  ny_noaa %>% 
  sample_n(2000) %>% 
  janitor::clean_names() %>%
  separate(date, sep = "-", into = c("year", "month", "day")) %>%
  mutate_at(c(2:9), as.numeric)


ny_noaa_samp
```

Real code


ny_noaa_clean <- ny_noaa %>% 
  janitor::clean_names() %>%
  separate(date, sep = "-", into = c("year", "month", "day")) 
  




