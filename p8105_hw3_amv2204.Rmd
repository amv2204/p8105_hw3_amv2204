---
title: "Homework 3"
author: "Ashwini Varghese"
date: "2022-10-17"
output: github_document
editor_options: 
  chunk_output_type: console
---

## Setup for coding

```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(hexbin)
library(ggridges)
library(patchwork)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Let's load the dataset:

```{r load_instacart}
data("instacart")

instacart
```

The dataset has 15 variables and 1,384,617 observations. Key variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. 

To find how many aisles there are as well as which aisles have the most items ordered from we can use the below code:

```{r aisles}
instacart %>% 
  summarize(n_distinct(aisle))

instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))

```

There are 134 aisles and the aisles where the most food came from are fresh vegetables, fresh fruits, and packaged vegetables and fruits. 

Now we will display these amounts in a plot and show the number of items ordered in each aisle.

```{r aisle_plot}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Here is a table showing the three most popular items in the following three aisles: baking ingredients, dog food care, packaged vegetables fruits

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

This last table shows the average hour of the day in which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```



## Problem 2

Let's load, tidy, and wrangling this dataset:

```{r accel_cleaning}
accel = read_csv("./accel_data.csv") %>% 
    janitor::clean_names() %>% 
    pivot_longer(
      activity_1:activity_1440,
      names_to = "minute",
      names_prefix = "activity_",
      values_to = "activity_count"
    ) %>% 
    mutate(day_type = if_else(day == "Saturday", "weekend",
                              if_else(day == "Sunday", "weekend", "weekday"))) %>%
    mutate(day = as.factor(day)) %>% 
    mutate(minute = as.numeric(minute)) %>% 
    mutate(day_type = as.factor(day_type)) %>% 
    mutate(day = fct_relevel(day, "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

accel
```

Description of dataset:

This dataset has 6 variable (`ncol(accel)`); `week`, `day`, and `day_id` were from the original file untouched besides cleaning up the names using `janitor::clean_names()` but the other 3 (`minute,` `activity_count`, and `day_type`) are new variables created using `pivot_longer` and `mutate`. This dataset has 50,400 observations, found using `nrow(accel)`.


Let's now make a new variable that is the total activity count for a given day and show it in a table:

```{r total_activity}
accel2 <- accel %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity) %>% 
  knitr::kable(digits = 1)

accel2
```

From this table, it is hard to determine if any trends are apparent over the days and by each week.

So let's then make a single panel that shows the 24-hour activity time courses for each day:

```{r accel_plot}
accel %>%
  ggplot(aes(x = minute, y = activity_count, color = day)) +
  geom_point(aes(alpha = .5)) +
   labs(
    title = "Activity counts by minute for each day",
    x = "Minute of the day",
    y = "Activity count"
  )
```

From this graph, we can see that the activity counts throughout any given day are usually not higher than about 1,875. The exceptions are at about the 400th minute, 560th minute, 650th minute, 1000th minute and 1,250th minute. The days of the week that we see these higher activity counts are on Thursdays, Fridays, Sundays, and Saturdays, respectively. At the 1,250th minute, on various days throughout the week, there is a large activity count. 

## Problem 3

Let's load the data:

```{r load_nynoaa}
data("ny_noaa")

ny_noaa
```

The dataset has 7 variables (`ncol(ny_noaa)`) and 2,595,176 observations (`nrow(ny_noaa)`). It has a combination of integer and character variables, with also a date variable. The variables that make up the dataset are an ID variable of the weather station, date of observation, precipitation (mm), snowfall (mm), snow depth (mm), and the maximum and minimum temperatures in Celsius. There is a large number of missing data because each weather station may collect only a subset of these variables, so the dataset has observations with missing data. 

Let's now do some data cleaning of this dataset:

```{r clean_nynoaa}
ny_noaa_clean <- ny_noaa %>% 
  janitor::clean_names() %>%
  separate(date, sep = "-", into = c("year", "month", "day")) %>%
  mutate_at(c(2:9), as.numeric) %>% 
  mutate(prcp = prcp/10) %>% 
  mutate(tmin = tmin/10, tmax = tmax/10) %>% 
  mutate(month = month.name[as.numeric(month)])
ny_noaa_clean
```

We cleaned up the data by cleaning the names, separating the variable for date of observation into the year, month, and day, converting all the variables except the ID into a numeric variable, and converting the `prcp`, `tmix`, and `tmax` variables from it's tenths value to it's whole value by dividing by 10.  

We will next find the most commonly observed value for `snowfall` using the `count` and `order` functions. 

```{r snowfall}

snowfall <- ny_noaa_clean %>% 
  count(snow, name = "n_obs")
  
snowfall <- snowfall[order(-snowfall$n_obs),] %>% 
  knitr::kable(digits = 1)

snowfall
```

We can see from the resulting table that the most observed value for `snowfall` is 0 mm. After that, we have 25 mm and the third highest value is 13 mm. These are the highest values maybe because the observations in this dataset were collected from weather stations in New York state and for a majority of the year, the weather is not optimal for snow.

Now let's make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r two_plot}
avgtmax <- ny_noaa_clean %>% 
  filter(
    month %in% c("January", "July")
  ) %>% 
  drop_na(tmax) %>% 
  group_by(year, id, month) %>% 
  summarize(
    avg_tmax = mean(tmax, na.rm = TRUE)
  ) 

ggplot(avgtmax, (aes(x = year, y = avg_tmax, color = id))) +
  geom_point() +
  geom_line() +
  labs(
    title = "Average maximum temperature vs. year by weather station in January and July",
    x = "Year",
    y = "Average maximum temperature (C)"
  ) +
  facet_grid(. ~ month) +
  theme(legend.position = "none")

```

The overall trends in these graphs are that in January, the average maximum temperature from 1980 to 2010 was between -10 and 10 degrees Celsius. We have a few outliers at about 11, -9, -13, and -12. In July, the average maximum temperature from 1980 to 2010 was between 20 and 35 degrees Celsius. Some of the outliers were 14, 18, 19, and 36.  


Now let's make a two-panel plot showing (i) tmax vs tmin for the full dataset and (ii) the distribution of snowfall values greater than 0 and less than 100 separately by year

```{r diff_two_plot}

temp_plot = 
ny_noaa_clean %>% 
ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
    labs(
    title = "Minimum and maximum temperatures",
    x = "Minimum temperature (C)",
    y = "Maximum temperature (C)"
    )


snow <- ny_noaa_clean %>% 
  filter(snow > 0 & snow < 100) %>%
  mutate(snow = as.numeric(snow)) %>%
  mutate(year = as.factor(year))

snow_plot =   
ggplot(snow, aes(x = snow, y = year)) +
  geom_density_ridges() +
  labs(
    title = "Snowfall values by year",
    x = "Snowfall (mm)",
    y = "Year"
    )

temp_plot + snow_plot
  
```

In these plots, we see that there is large number of days in which the maximum temperature and minimum temperatures were between 15 for tmin and 30 for tmax and -15 for tmin and -5 for tmax. For the snowfall plot, from 1981 to 2010, most of the days with snowfall have a value between 0 and 30 mm. There is also another large set of days that had snowfall values between 40 and 60 mm and another between 70 and 80 mm.
  
